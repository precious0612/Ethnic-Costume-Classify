{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **民族服饰分类**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "bad_list=[]\n",
    "dir=r'./images'\n",
    "subdir_list=os.listdir(dir) # create a list of the sub directories in the directory ie train or test\n",
    "for d in subdir_list:  # iterate through the sub directories train and test\n",
    "    dpath=os.path.join (dir, d) # create path to sub directory\n",
    "    if d in ['test', 'train']:\n",
    "        class_list=os.listdir(dpath) # list of classes ie dog or cat\n",
    "       # print (class_list)\n",
    "        for klass in class_list: # iterate through the two classes\n",
    "            class_path=os.path.join(dpath, klass) # path to class directory\n",
    "            #print(class_path)\n",
    "            if class_path == dpath+'/.DS_Store':\n",
    "                os.remove(class_path)\n",
    "                continue\n",
    "            file_list=os.listdir(class_path) # create list of files in class directory\n",
    "            for f in file_list: # iterate through the files\n",
    "                fpath=os.path.join (class_path,f)\n",
    "                index=f.rfind('.') # find index of period infilename\n",
    "                ext=f[index+1:] # get the files extension\n",
    "                if ext  not in ['jpg', 'png', 'bmp', 'gif']:\n",
    "                    print(f'file {fpath}  has an invalid extension {ext}')\n",
    "                    os.remove(fpath)\n",
    "                    print(\"Successfully removed!\")\n",
    "                    bad_list.append(fpath)\n",
    "                else:\n",
    "                    try:\n",
    "                        img=cv2.imread(fpath)\n",
    "                        size=img.shape\n",
    "                    except:\n",
    "                        print(f'file {fpath} is not a valid image file ')\n",
    "                        os.remove(fpath)\n",
    "                        print(\"Successfully removed!\")\n",
    "                        bad_list.append(fpath)\n",
    "\n",
    "print (bad_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "shift = 0.3\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.5,\n",
    "    zoom_range=0.2,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=shift, height_shift_range=shift,\n",
    "    featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    zca_whitening=True,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2607 images belonging to 56 classes.\n",
      "Found 1105 images belonging to 56 classes.\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "batch_size = 25\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    # './images_changed/train',\n",
    "    './images/train',\n",
    "    # target_size=(300, 300),\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    # './images_changed/test',\n",
    "    './images/test',\n",
    "    # target_size=(150, 150),\n",
    "    batch_size=batch_size,\n",
    "    # class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet-18\n",
    "Reference:\n",
    "[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n",
    "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\n",
    "Surpassing human-level performance on imagenet classification. In\n",
    "ICCV, 2015.\n",
    "\"\"\"\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Add, Activation, ZeroPadding2D, Dropout, AveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "class Conv(Model):\n",
    "\n",
    "    def __init__(self, channels: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = Conv2D(channels,kernel_size=3,use_bias=False)\n",
    "\n",
    "    def call(self, input):\n",
    "\n",
    "        return self.conv(input)\n",
    "\n",
    "class dense_block(Model):\n",
    "\n",
    "    def __init__(self,nb_layers: int, nb_filters: int, dropout_rate=None, grop_rate=0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.__nb_layers = nb_layers\n",
    "        self.__channels = nb_filters\n",
    "        self.__dropout_rate = dropout_rate\n",
    "        self.__grop_rate = grop_rate\n",
    "\n",
    "        self.zero_pool = ZeroPadding2D()\n",
    "        self.conv = Conv2D(self.__channels,kernel_size=3,use_bias=False)\n",
    "        self.bn = BatchNormalization()\n",
    "        self.drop_out = Dropout(self.__dropout_rate)\n",
    "\n",
    "    def call(self, input):\n",
    "        concat_feat = input\n",
    "\n",
    "        for _ in range(self.__nb_layers):\n",
    "            x = self.zero_pool(input)\n",
    "            x = self.conv(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = self.bn(x)\n",
    "\n",
    "            if self.__dropout_rate:\n",
    "                x = self.drop_out(x)\n",
    "\n",
    "            concat_feat =  layers.Concatenate()([x,concat_feat])\n",
    "\n",
    "            if self.__grop_rate!=0:\n",
    "                self.__channels += self.__grop_rate\n",
    "\n",
    "        return concat_feat\n",
    "\n",
    "class transition_block(Model):\n",
    "\n",
    "    def __init__(self, channels: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv = Conv2D(self.__channels,1,1,use_bias=False)\n",
    "        self.avg = AveragePooling2D(2,2)\n",
    "\n",
    "    def call(self, input):\n",
    "\n",
    "        x = self.bn_1(input)\n",
    "        x = Activation('relu')(x)\n",
    "        x = self.conv(x)\n",
    "        out = self.avg(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResnetBlock(Model):\n",
    "    \"\"\"\n",
    "    A standard resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "        self.merge = Add()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        x = self.merge([x, res])\n",
    "        out = Activation('relu')(x)\n",
    "        return out\n",
    "\n",
    "class ResNet18(Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = Conv2D(64, (7, 7), strides=2,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        self.res_1_1 = ResnetBlock(64)\n",
    "        self.res_1_2 = ResnetBlock(64)\n",
    "        self.res_2_1 = ResnetBlock(128, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(128)\n",
    "        self.res_3_1 = ResnetBlock(256, down_sample=True)\n",
    "        self.res_3_2 = ResnetBlock(256)\n",
    "        self.res_4_1 = ResnetBlock(512, down_sample=True)\n",
    "        self.res_4_2 = ResnetBlock(512)\n",
    "\n",
    "        self.den_1_1 = dense_block(nb_layers=4,nb_filters=64,dropout_rate=0.2,grop_rate=32)\n",
    "        self.trans_1 = transition_block(256)\n",
    "        self.den_1_2 = dense_block(nb_layers=4,nb_filters=128,dropout_rate=0.2,grop_rate=32)\n",
    "        self.trans_2 = transition_block(512)\n",
    "\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "        self.flat = Flatten()\n",
    "        self.fc = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        out = Activation('relu')(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n",
    "            out = res_block(out)\n",
    "\n",
    "        out = self.den_1_1(out)\n",
    "        out = self.trans_1(out)\n",
    "\n",
    "        #第二个块\n",
    "        out = self.den_1_2(out)\n",
    "        out = self.trans_2(out)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: AMD Radeon Pro 5500M\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 3.99 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 21:06:55.848598: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-09 21:06:55.850114: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method ResnetBlock.call of <__main__.ResnetBlock object at 0x7fa4464a46a0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ResnetBlock.call of <__main__.ResnetBlock object at 0x7fa4464a46a0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method dense_block.call of <__main__.dense_block object at 0x7fa446877be0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method dense_block.call of <__main__.dense_block object at 0x7fa446877be0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"res_net18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  9472      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  multiple                 256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block (ResnetBlock)  multiple                  74368     \n",
      "                                                                 \n",
      " resnet_block_1 (ResnetBlock  multiple                 74368     \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_2 (ResnetBlock  multiple                 231296    \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_3 (ResnetBlock  multiple                 296192    \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_4 (ResnetBlock  multiple                 921344    \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_5 (ResnetBlock  multiple                 1182208   \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_6 (ResnetBlock  multiple                 3677696   \n",
      " )                                                               \n",
      "                                                                 \n",
      " resnet_block_7 (ResnetBlock  multiple                 4723712   \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_block (dense_block)   multiple                  295168    \n",
      "                                                                 \n",
      " transition_block (transitio  multiple                 199680    \n",
      " n_block)                                                        \n",
      "                                                                 \n",
      " dense_block_1 (dense_block)  multiple                 295424    \n",
      "                                                                 \n",
      " transition_block_1 (transit  multiple                 396288    \n",
      " ion_block)                                                      \n",
      "                                                                 \n",
      " global_average_pooling2d (G  multiple                 0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  28728     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,406,200\n",
      "Trainable params: 12,393,144\n",
      "Non-trainable params: 13,056\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18(56)\n",
    "# model = DenseNet121(\n",
    "#     classes=56,\n",
    "#     weights=None,\n",
    "#     input_shape=(256, 256, 3),\n",
    "#     # pooling=None,\n",
    "#     # name=\"DenseNet121\",\n",
    "# )\n",
    "model.build(input_shape = (None,)+train_generator.image_shape)\n",
    "#use categorical_crossentropy since the label is one-hot encoded\n",
    "from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04) #parameters suggested by He [1]\n",
    "model.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/precious/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/keras/preprocessing/image.py:1663: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/Users/precious/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/keras/preprocessing/image.py:1682: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 21:07:32.286703: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18/105 [====>.........................] - ETA: 2:12 - loss: 4.6383 - accuracy: 0.0178"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m es \u001b[39m=\u001b[39m EarlyStopping(patience\u001b[39m=\u001b[39m \u001b[39m8\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m#I did not use cross validation, so the validate performance is not accurate.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# STEPS = len(X_train) / batch_size\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mvalidation_generator,\n\u001b[1;32m      8\u001b[0m                     \u001b[39m# callbacks=[es]\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m                     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow_metal/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "import scipy\n",
    "\n",
    "es = EarlyStopping(patience= 8, restore_best_weights=True, monitor=\"val_accuracy\")\n",
    "#I did not use cross validation, so the validate performance is not accurate.\n",
    "# STEPS = len(X_train) / batch_size\n",
    "history = model.fit(train_generator, epochs=150, validation_data=validation_generator,\n",
    "                    # callbacks=[es]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 借助 history 对象了解训练过程\n",
    "history_dict = history.history\n",
    "acc = history_dict['accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "# 借助 Matplotlib 绘制图像\n",
    "plt.plot(range(1, len(acc)+1), acc, 'b--')\n",
    "plt.plot(range(1, len(loss)+1), loss, 'r-')\n",
    "# plt.plot(range(1, len(val_loss)+1), val_loss, '-')\n",
    "plt.plot(range(1, len(val_accuracy)+1), val_accuracy, '--')\n",
    "\n",
    "# 显示图例\n",
    "plt.legend(['accuracy', 'loss'\n",
    "               , 'val_accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
